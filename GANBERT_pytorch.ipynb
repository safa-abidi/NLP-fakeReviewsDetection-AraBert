{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safa-abidi/NLP-fakeReviewsDetection-AraBert/blob/main/GANBERT_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0m5KR34gmRH"
      },
      "source": [
        "Let's GO!\n",
        "\n",
        "Required Imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "43e4b5e1-0cde-48f1-bff7-1b9cfd50e629"
      },
      "source": [
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (3.12.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (23.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.3.2) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.3.2) (2022.12.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.3.2) (1.2.0)\n",
            "Building wheels for collected packages: tokenizers, sacremoses\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=1fcc6597f02dfe786af158de0b4e4973b9e7c99c59ed90c85215c88a4a0a0694\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Failed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeZgRup520II",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0028eb7-93f6-43df-c555-26dce7028a99"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn346xVjVykF",
        "outputId": "8e7b07b9-192c-4746-914c-17d673310f6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "QfMJUNbucOrt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS_PATH = '/content/drive/MyDrive/'\n",
        "\n",
        "%cd /content/drive/MyDrive/datasets\n",
        "\n",
        "# Labeled Dataset\n",
        "labeled_dataset_path = 'dataset_arabic_translated/Merged_Labelled_Arabic_fake_news_Dataset.xlsx'\n",
        "labeled_dataset = pd.read_excel(labeled_dataset_path)\n",
        "#labeled_dataset = labeled_dataset.sample(n=20)\n",
        "print(labeled_dataset)\n",
        "\n",
        "# Unlabeled Dataset\n",
        "unlabeled_dataset_path = 'SemEval2016_arabic/AR_Hotels_Train_SB1.xml'\n",
        "tree = ET.parse(unlabeled_dataset_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Extract reviews from XML and add to list of tuples\n",
        "reviews = []\n",
        "for review in root.iter('Review'):\n",
        "    for sentence in review.iter('sentence'):\n",
        "        text = sentence.find('text').text.strip()\n",
        "        label = -1\n",
        "        reviews.append((text, label))\n",
        "\n",
        "# Create a pandas DataFrame from the list of tuples\n",
        "unlabeled_dataset = pd.DataFrame(reviews, columns=['Review', 'Class'])\n",
        "#unlabeled_dataset = unlabeled_dataset.sample(n=20)\n",
        "print(\"Unlabeled Dataset\",unlabeled_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdldK0hlV5J7",
        "outputId": "39ad12ff-306f-4bba-c78b-d0cf0c8a7bff"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n",
            "            Hotel name                                             Review  \\\n",
            "1110             James  لقد وجدت هذا الفندق الرائع! الموقع رائع ، على ...   \n",
            "33             Affinia  كنت أبحث عن فندق رائع غير سلسلة لقضاء عطلة نها...   \n",
            "243            Allegro  كنت غير راضٍ للغاية عن إقامتي في Allegro. يبدو...   \n",
            "130             Monaco  أثناء دخولي إلى الفندق ، تم الترحيب بي بحرارة....   \n",
            "358             Amalfi  غالبًا ما أزور شيكاغو ، لكن هذه كانت إقامتي ال...   \n",
            "344             Amalfi  أقمت في فندق Amalfi في شيكاغو هذا الشهر مع توق...   \n",
            "1085             James  فندق جيمس في شيكاغو هو إلى حد بعيد أحد أفضل ال...   \n",
            "1014  InterContinental  فندق إنتركونتيننتال هو بالفعل كنز مخفي. يقع ال...   \n",
            "1028  InterContinental  أقمنا هنا عبر Hotwire وحصلنا على سعر منخفض بشك...   \n",
            "494            Concrad  أقمنا أنا وزوجي مؤخرًا في Conrad Chicago لمدة ...   \n",
            "453         Ambassador  رائع! أحببت فندق Ambassador إيست في شيكاغو! كا...   \n",
            "22             Affinia  أقمت في Fitzpatrick نتيجة لجميع المراجعات المت...   \n",
            "315            Allegro  إذا كنت في شيكاغو وترغب في الإقامة في The Loop...   \n",
            "605           Fairmont  لقد عدت لتوي من إقامتي الأولى في فيرمونت شيكاغ...   \n",
            "182            Sofitel  بعد قراءة ملاحظات مرشد الرحلة ، كنت أتوقع إقام...   \n",
            "794             Hilton  مكثت في فندق Chicago Hilton لمدة ثلاث ليالٍ ، ...   \n",
            "266            Allegro  مرت عمليات تسجيل الوصول بسلاسة. ولكن يرجى العل...   \n",
            "591           Fairmont  بشكل عام فنادق فيرمونت هي خياري المفضل ، ولكن ...   \n",
            "1335            Palmer  لقد تعثرت أنا وزوجي كثيرًا في رحلة نهاية الأسب...   \n",
            "359             Amalfi  بقينا في أمالفي في يوليو 2010 ، الرابع من يولي...   \n",
            "\n",
            "     Source Opinion  Class  Unnamed: 5  \n",
            "1110    Web  إيجابي  صادقة         NaN  \n",
            "33      Web     نفي  صادقة         NaN  \n",
            "243   Mturk     نفي   خادع         NaN  \n",
            "130   Mturk  إيجابي   خادع         NaN  \n",
            "358     Web     نفي  صادقة         NaN  \n",
            "344     Web     نفي  صادقة         NaN  \n",
            "1085  Mturk  إيجابي   خادع         NaN  \n",
            "1014  Mturk  إيجابي   خادع         NaN  \n",
            "1028    Web  إيجابي  صادقة         NaN  \n",
            "494   Mturk     نفي   خادع         NaN  \n",
            "453   Mturk  إيجابي   خادع         NaN  \n",
            "22      Web     نفي  صادقة         NaN  \n",
            "315     Web  إيجابي  صادقة         NaN  \n",
            "605   Mturk  إيجابي   خادع         NaN  \n",
            "182     Web     نفي  صادقة         NaN  \n",
            "794     Web  إيجابي  صادقة         NaN  \n",
            "266     Web     نفي  صادقة         NaN  \n",
            "591     Web     نفي  صادقة         NaN  \n",
            "1335  Mturk  إيجابي   خادع         NaN  \n",
            "359     Web     نفي  صادقة         NaN  \n",
            "Unlabeled Dataset                                                  Review  Class\n",
            "2378  النقاط البارزة: سيرج جعل الذكرى لا تنسى نمن خل...     -1\n",
            "4343     الطعام متوسط ولا طعم له. لن نقيم فيه مرة أخرى.     -1\n",
            "3240  أجازه رائعة الفندق أكثر من رائع وكل شيء في الف...     -1\n",
            "3105  السعر مبالغ فيه، التعامل شبه سيء المدير لم يصح...     -1\n",
            "1197  يتميز الفندق بالموقع الرائع والديكور الفخم وال...     -1\n",
            "2955  الغرف في  أول سيزونز نورثبريدج   شعرت أن الغرف...     -1\n",
            "3332                           تحتاج الغرف إلى التنظيف.     -1\n",
            "2171                     الخمه لا توازي الدفع من الزبون     -1\n",
            "2541  ذهبت لمنتجع شاطيء سلطانه في 27 أكتوبر مع عائلت...     -1\n",
            "3536       فندق لا يستحق حتي نجمة واحدة،قديم جداً جداً،     -1\n",
            "3446  لا يوجد اتصال بالانترنت بالغرفة، الواي فاي غال...     -1\n",
            "173   تمتع فريقنا الصغير بواحدة من أرقى الوجبات التي...     -1\n",
            "1364  الجانب السلبي الوحيد هو بأنه يقع على شارع رئيس...     -1\n",
            "2149  فريق عمل الفندق ودود جدًا... إنهم يبذلون جهودا...     -1\n",
            "4348  ويوجد مصلى بالفندق يتبع للحرم اي انك تصلي فيه ...     -1\n",
            "816   طلبت ليموزين للمطار احضر تاكسي عادي من امام ال...     -1\n",
            "1784  إقامة رائعة تتميز أجنحة رافلز بالإطلالة الرائع...     -1\n",
            "970   هذا الفندق بشع ويجب عليكم تجنبه. تم التقاط الص...     -1\n",
            "1171                       قديم للغاية ولا يواكب العصر!     -1\n",
            "3261      والموقع بالقرب من وسط المدينة والأماكن المهمة     -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# data split labeled_training and test\n",
        "df_train_labeled, df_test = train_test_split(labeled_dataset, train_size=0.7, random_state=0)\n",
        "print('Size of train dataframe: ', df_train_labeled.shape[0])\n",
        "print('Size of test dataframe: ', df_test.shape[0])\n",
        "\n",
        "## Encode the dataset labels\n",
        "df_test['label'] =  df_test.apply(lambda x: 0 if x['Class'] == 'خادع' else 1 if x['Class'] == 'صادقة' else -1, axis=1)\n",
        "df_train_labeled['label'] = df_train_labeled.apply(lambda x: 0 if x['Class'] == 'خادع' else 1 if x['Class'] == 'صادقة' else -1, axis=1)\n",
        "\n",
        "#Load the examples\n",
        "labeled_examples = df_train_labeled\n",
        "unlabeled_examples = unlabeled_dataset\n",
        "test_examples = df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWSzQYd5emAa",
        "outputId": "8a11061e-a102-42a9-8c38-51eb20df6d62"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train dataframe:  14\n",
            "Size of test dataframe:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labeled_examples.label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izFNK0uWg5r6",
        "outputId": "6d815bf6-66ce-4d1a-9489-5f5e2de024c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1253    0\n",
            "1569    0\n",
            "937     0\n",
            "168     0\n",
            "1544    1\n",
            "584     1\n",
            "650     0\n",
            "237     1\n",
            "30      1\n",
            "1103    1\n",
            "135     0\n",
            "526     0\n",
            "65      1\n",
            "968     0\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw0HC_hU3FUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9fd278-5a46-4f14-8508-c0972864c549"
      },
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator, \n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1; \n",
        "# number of hidden layers in the discriminator, \n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1; \n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets, \n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 10\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "! git clone https://github.com/crux82/ganbert\n",
        "\n",
        "\n",
        "label_list = [0,1,-1]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ganbert' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from transformers import AutoModel"
      ],
      "metadata": {
        "id": "NGk05svPXl2Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxghkkZq3Gbn",
        "outputId": "8d45d1a0-2aef-4a32-be72-76e35107a3b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/pytorch_model.bin\n",
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of BertModel were initialized from the model checkpoint at aubmindlab/bert-base-arabertv2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/vocab.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are \n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples  \n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask: \n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples): \n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "  \n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization \n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "  \n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids) \n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset), \n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_examples = list(zip(df_train_labeled['Review'].tolist(), df_train_labeled['label'].tolist()))\n",
        "test_examples = list(zip(df_test['Review'].tolist(), df_test['label'].tolist()))\n",
        "\n",
        "unlabeled_examples = list(zip(unlabeled_dataset['Review'].tolist(), [-1 for _ in range(len(unlabeled_dataset['Review']))]))\n",
        "print(unlabeled_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc7EgnC0dPrx",
        "outputId": "792f70fc-a04d-43cc-9f89-f2917c49022d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('النقاط البارزة: سيرج جعل الذكرى لا تنسى نمن خلال فعاليته الممتازة والرعاية والاهتمام. جامع الفناء في الليل. الأسواق. الطعام. الوقت الذي لا يُنسى - زيارة جبال الأطلس (للحجز يجب الاتصال بفيليب في موقع ليجيند سيفازيونز).', -1), ('الطعام متوسط ولا طعم له. لن نقيم فيه مرة أخرى.', -1), ('أجازه رائعة الفندق أكثر من رائع وكل شيء في الفندق يجذب الانتباه إلى الجمال والروعة', -1), ('السعر مبالغ فيه، التعامل شبه سيء المدير لم يصحح الخطأ الذي وقع من الموظفين، يعني يعطيك إحساس انك ساكن عندهم مجاناً', -1), ('يتميز الفندق بالموقع الرائع والديكور الفخم والغرف الواسعة وذات التصميم الجميل والمريح ،', -1), ('الغرف في  أول سيزونز نورثبريدج \\xa0 شعرت أن الغرف يبدو عليها القِدم نوعًا ما. الصندوق الموجود فوق النافذة جعلني أشعر بشعور خانق نوعًا ما. وحيث أنني أنام على جنبي بالقرب من حافة السرير فقد جعلني السرير أشعر كما لو أنني سأسقط من عليه.', -1), ('تحتاج الغرف إلى التنظيف.', -1), ('الخمه لا توازي الدفع من الزبون', -1), ('ذهبت لمنتجع شاطيء سلطانه في 27 أكتوبر مع عائلتي. حجزت غرفتان 126 و 130. الخدمة كانت سيئة جداً جداً، الغرف ليست نظيفة على الإطلاق، لا يوجد خدمة الغرف، الحمام ليس نظيفاً، لا يوجد ورق مرحاض.', -1), ('فندق لا يستحق حتي نجمة واحدة،قديم جداً جداً،', -1), ('لا يوجد اتصال بالانترنت بالغرفة، الواي فاي غال جدًا وموجود في اللوبي فقط -', -1), ('تمتع فريقنا الصغير بواحدة من أرقى الوجبات التي تناولانها في جميع أنحاء مصر هذا المساء بعد عرض الصوت والضوء.', -1), ('الجانب السلبي الوحيد هو بأنه يقع على شارع رئيسي وذلك يسبب بعض الإزعاج.', -1), ('فريق عمل الفندق ودود جدًا... إنهم يبذلون جهوداً إضافية لجعلك تشعر كما لو أنك في المنزل. .', -1), ('ويوجد مصلى بالفندق يتبع للحرم اي انك تصلي فيه مع امام الحرم', -1), ('طلبت ليموزين للمطار احضر تاكسي عادي من امام الفندق و اخذ عمولة من السائق', -1), ('إقامة رائعة تتميز أجنحة رافلز بالإطلالة الرائعة على الكعبة المشرفة..', -1), ('هذا الفندق بشع ويجب عليكم تجنبه. تم التقاط الصور جيدًا لكنها ليست حقيقة.', -1), ('قديم للغاية ولا يواكب العصر!', -1), ('والموقع بالقرب من وسط المدينة والأماكن المهمة', -1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "print(label_map)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UZT1K7ImPUc",
        "outputId": "51fdd5e5-328a-477b-a995-e0efacadea69"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0, 1: 1, -1: 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c-nsMXlKX-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43255b78-c025-428a-e87c-fca1d40c5df5"
      },
      "source": [
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = labeled_examples\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "#If unlabel examples are available\n",
        "if unlabeled_examples:\n",
        "  train_examples = train_examples + unlabeled_examples\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "#------------------------------\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-b5617700c116>:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  label_mask_array = torch.tensor(label_mask_array)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ihcw3vquaQm"
      },
      "source": [
        "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "source": [
        "#------------------------------\n",
        "#   The Generator as in \n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uje9s2zQunFc"
      },
      "source": [
        "We instantiate the Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylz5rvqE3U2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5eede35-261f-4d55-b2c9-c9b31b74e0c4"
      },
      "source": [
        "# The config file is required to get the dimension of the vector produced by \n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "# print(config)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv2/snapshots/9176b29e90eb3c8549463700c9a7415bac80d0bc/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv2\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3qzp2-usZE"
      },
      "source": [
        "Let's go with the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhqylHGK3Va4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc241bd-7a0a-4745-bc0a-02c90f1a8154"
      },
      "source": [
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "#scheduler\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    transformer.train() \n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every print_each_n_step batches.\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "     \n",
        "        # Encode real data in the Transformer\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        \n",
        "        # Generate fake data that should have the same distribution of the ones\n",
        "        # encoded by the transformer. \n",
        "        # First noisy input are used in input to the Generator\n",
        "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "        # Gnerate Fake data\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        # Generate the output of the Discriminator for real and fake data.\n",
        "        # First, we put together the output of the tranformer and the generator\n",
        "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        # Then, we select the output of the disciminator\n",
        "        features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "        # Finally, we separate the discriminator's output for the real and fake\n",
        "        # data\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features = features_list[0]\n",
        "        D_fake_features = features_list[1]\n",
        "      \n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits = logits_list[0]\n",
        "        D_fake_logits = logits_list[1]\n",
        "        \n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs = probs_list[0]\n",
        "        D_fake_probs = probs_list[1]\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "        # Generator's LOSS estimation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "  \n",
        "        # Disciminator's LOSS estimation\n",
        "        logits = D_real_logits[:,0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        # The discriminator provides an output for labeled and unlabeled real data\n",
        "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        # It may be the case that a batch does not contain labeled examples, \n",
        "        # so the \"supervised loss\" in this case is not evaluated\n",
        "        if labeled_example_count == 0:\n",
        "          D_L_Supervised = 0\n",
        "        else:\n",
        "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                 \n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        # Calculate weigth updates\n",
        "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward() \n",
        "        \n",
        "        # Apply modifications\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        # A detail log of the individual losses\n",
        "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "        #             g_loss_d, g_feat_reg))\n",
        "\n",
        "        # Save the losses to print them later\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "          scheduler_d.step()\n",
        "          scheduler_g.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #     TEST ON THE EVALUATION DATASET\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our test set.\n",
        "    print(\"\")\n",
        "    print(\"Running Test...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    transformer.eval() #maybe redundant\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "   \n",
        "    total_test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "\n",
        "    #loss\n",
        "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "            filtered_logits = logits[:,0:-1]\n",
        "            # Accumulate the test loss.\n",
        "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "            \n",
        "        # Accumulate the predictions and the input labels\n",
        "        _, preds = torch.max(filtered_logits, 1)\n",
        "        all_preds += preds.detach().cpu()\n",
        "        all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    test_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "    print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.344\n",
            "  Average training loss discriminator: 2.863\n",
            "  Training epcoh took: 0:00:47\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.954\n",
            "  Test took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.350\n",
            "  Average training loss discriminator: 2.602\n",
            "  Training epcoh took: 0:00:40\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.860\n",
            "  Test took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.358\n",
            "  Average training loss discriminator: 2.357\n",
            "  Training epcoh took: 0:00:38\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.800\n",
            "  Test took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.375\n",
            "  Average training loss discriminator: 2.213\n",
            "  Training epcoh took: 0:00:37\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.765\n",
            "  Test took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.389\n",
            "  Average training loss discriminator: 2.075\n",
            "  Training epcoh took: 0:00:40\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.740\n",
            "  Test took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.402\n",
            "  Average training loss discriminator: 2.071\n",
            "  Training epcoh took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.716\n",
            "  Test took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.421\n",
            "  Average training loss discriminator: 1.910\n",
            "  Training epcoh took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.667\n",
            "  Test Loss: 0.694\n",
            "  Test took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.432\n",
            "  Average training loss discriminator: 1.813\n",
            "  Training epcoh took: 0:00:37\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.833\n",
            "  Test Loss: 0.688\n",
            "  Test took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.450\n",
            "  Average training loss discriminator: 1.742\n",
            "  Training epcoh took: 0:00:37\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.833\n",
            "  Test Loss: 0.697\n",
            "  Test took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss generetor: 0.466\n",
            "  Average training loss discriminator: 1.595\n",
            "  Training epcoh took: 0:00:39\n",
            "\n",
            "Running Test...\n",
            "  Accuracy: 0.833\n",
            "  Test Loss: 0.665\n",
            "  Test took: 0:00:01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDm9NProRB4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d79951-2ac6-42fd-bff1-c23323554254"
      },
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 1, 'Training Loss generator': 0.34390881657600403, 'Training Loss discriminator': 2.863114833831787, 'Valid. Loss': 0.9542903900146484, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:47', 'Test Time': '0:00:02'}\n",
            "{'epoch': 2, 'Training Loss generator': 0.3499678075313568, 'Training Loss discriminator': 2.601722240447998, 'Valid. Loss': 0.8599696159362793, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:40', 'Test Time': '0:00:02'}\n",
            "{'epoch': 3, 'Training Loss generator': 0.3577694594860077, 'Training Loss discriminator': 2.3567676544189453, 'Valid. Loss': 0.8002236485481262, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:38', 'Test Time': '0:00:01'}\n",
            "{'epoch': 4, 'Training Loss generator': 0.3752470016479492, 'Training Loss discriminator': 2.2133126258850098, 'Valid. Loss': 0.7651305794715881, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:37', 'Test Time': '0:00:01'}\n",
            "{'epoch': 5, 'Training Loss generator': 0.38866156339645386, 'Training Loss discriminator': 2.0748705863952637, 'Valid. Loss': 0.7403874397277832, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:40', 'Test Time': '0:00:01'}\n",
            "{'epoch': 6, 'Training Loss generator': 0.40183958411216736, 'Training Loss discriminator': 2.0711305141448975, 'Valid. Loss': 0.716319739818573, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:39', 'Test Time': '0:00:01'}\n",
            "{'epoch': 7, 'Training Loss generator': 0.42122483253479004, 'Training Loss discriminator': 1.9095481634140015, 'Valid. Loss': 0.6936162114143372, 'Valid. Accur.': 0.6666666666666666, 'Training Time': '0:00:39', 'Test Time': '0:00:02'}\n",
            "{'epoch': 8, 'Training Loss generator': 0.43216854333877563, 'Training Loss discriminator': 1.8133964538574219, 'Valid. Loss': 0.6876615881919861, 'Valid. Accur.': 0.8333333333333334, 'Training Time': '0:00:37', 'Test Time': '0:00:01'}\n",
            "{'epoch': 9, 'Training Loss generator': 0.44956153631210327, 'Training Loss discriminator': 1.7416455745697021, 'Valid. Loss': 0.6969605088233948, 'Valid. Accur.': 0.8333333333333334, 'Training Time': '0:00:37', 'Test Time': '0:00:02'}\n",
            "{'epoch': 10, 'Training Loss generator': 0.46600329875946045, 'Training Loss discriminator': 1.595198631286621, 'Valid. Loss': 0.6645954251289368, 'Valid. Accur.': 0.8333333333333334, 'Training Time': '0:00:39', 'Test Time': '0:00:01'}\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:07:57 (h:mm:ss)\n"
          ]
        }
      ]
    }
  ]
}