{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safa-abidi/NLP-sentimentAnalysis-AraBert/blob/main/co_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "L0D-38rHTB7k"
      },
      "id": "L0D-38rHTB7k"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "523f70b1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "id": "523f70b1"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, List, Callable, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import torch"
      ],
      "metadata": {
        "id": "mxik8dt3S8RE"
      },
      "id": "mxik8dt3S8RE",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ENOVRcdlVV00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2a6ddc-8c69-4007-fdb4-6df311e270fc"
      },
      "id": "ENOVRcdlVV00",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "dnnqHVVPVWJe"
      },
      "id": "dnnqHVVPVWJe",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "id": "0PGSFbx9XXcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb548ea-0c35-4692-9f9d-f678c84f4862"
      },
      "id": "0PGSFbx9XXcl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset: labelled and unlabled"
      ],
      "metadata": {
        "id": "XbMQ_lKBSR9k"
      },
      "id": "XbMQ_lKBSR9k"
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS_PATH = '/content/drive/MyDrive/'\n",
        "\n",
        "%cd /content/drive/MyDrive/datasets\n",
        "\n",
        "# Labeled Dataset\n",
        "labeled_dataset_path = 'dataset_arabic_translated/Merged_Labelled_Arabic_fake_news_Dataset.xlsx'\n",
        "labeled_dataset = pd.read_excel(labeled_dataset_path)\n",
        "print(\"labeled dataset\", labeled_dataset.shape)\n",
        "\n",
        "# Unlabeled Dataset\n",
        "unlabeled_dataset_path = 'SemEval2016_arabic/AR_Hotels_Train_SB1.xml'\n",
        "tree = ET.parse(unlabeled_dataset_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "###################################\n",
        "# New unlabelled dataset to add the the old one\n",
        "new_unlabeled_dataset_path = 'Merged_Unlabelled_Arabic_scraped_Hotel reviews.xlsx'\n",
        "new_unlabeled_dataset = pd.read_excel(new_unlabeled_dataset_path, sheet_name=1)\n",
        "print(\"new unlabeled dataset \", new_unlabeled_dataset.shape)\n",
        "###################################\n",
        "\n",
        "# Extract reviews from XML and add to list of tuples\n",
        "reviews = []\n",
        "for review in root.iter('Review'):\n",
        "    for sentence in review.iter('sentence'):\n",
        "        text = sentence.find('text').text.strip()\n",
        "        label = -1\n",
        "        reviews.append((text, label))\n",
        "\n",
        "# Create a pandas DataFrame from the list of tuples\n",
        "unlabeled_dataset1 = pd.DataFrame(reviews, columns=['Review', 'Class'])\n",
        "unlabeled_dataset = pd.concat([unlabeled_dataset1, new_unlabeled_dataset], axis=0)\n",
        "\n",
        "print(\"Unlabeled Dataset 1\",unlabeled_dataset1.shape)\n",
        "print(\"all unlabeled \", unlabeled_dataset.shape)\n"
      ],
      "metadata": {
        "id": "PTNtkC8bVxiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e232d3-439f-4495-f15e-3ce85f6cb142"
      },
      "id": "PTNtkC8bVxiT",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets\n",
            "labeled dataset (1600, 6)\n",
            "new unlabeled dataset  (799, 3)\n",
            "Unlabeled Dataset 1 (4802, 2)\n",
            "all unlabeled  (5601, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Encode the dataset labels\n",
        "labeled_dataset['Label'] = labeled_dataset.apply(lambda x: 0 if x['Class'] == 'خادع' else 1 if x['Class'] == 'صادقة' else -1, axis=1)\n",
        "print(labeled_dataset.shape)\n",
        "print(labeled_dataset)"
      ],
      "metadata": {
        "id": "T9Kv6YIiRLcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aebec42-beb4-4be3-e101-cca89f9f3db0"
      },
      "id": "T9Kv6YIiRLcT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600, 7)\n",
            "     Hotel name                                             Review Source  \\\n",
            "0       Affinia  فندق Affinia Chicago هو أحد أسوأ الفنادق التي ...  Mturk   \n",
            "1       Affinia  مكثت في Affina Chicago للاحتفال بالذكرى السنوي...  Mturk   \n",
            "2       Affinia  من الواضح أن فندق Affinia في شيكاغو يخدم ضيوف ...  Mturk   \n",
            "3       Affinia  محبط للغاية في إقامتنا. هذا الفندق لا يشبه ما ...  Mturk   \n",
            "4       Affinia  كان فندق Affinia في شيكاغو أحد أكثر الفنادق إث...  Mturk   \n",
            "...         ...                                                ...    ...   \n",
            "1595    Talbolt  الموقع والموقع والموقع. يا له من اكتشاف رائع! ...  الويب   \n",
            "1596    Talbolt  أقمنا هنا بسبب كل التقييمات الرائعة ، وكلها صح...  الويب   \n",
            "1597    Talbolt  مكثت في Talbott مرتين في الأسابيع القليلة الما...  الويب   \n",
            "1598    Talbolt  أقمنا أنا وزوجي هنا لمدة ثلاث ليال أثناء زيارة...  الويب   \n",
            "1599    Talbolt  مكثت 5 ليال في هذا الفندق الرائع. الغرف المحدث...  الويب   \n",
            "\n",
            "     Opinion  Class  Unnamed: 5  Label  \n",
            "0        نفي   خادع         NaN      0  \n",
            "1        نفي   خادع         NaN      0  \n",
            "2        نفي   خادع         NaN      0  \n",
            "3        نفي   خادع         NaN      0  \n",
            "4        نفي   خادع         NaN      0  \n",
            "...      ...    ...         ...    ...  \n",
            "1595  إيجابي  صادقة         NaN      1  \n",
            "1596  إيجابي  صادقة         NaN      1  \n",
            "1597  إيجابي  صادقة         NaN      1  \n",
            "1598  إيجابي  صادقة         NaN      1  \n",
            "1599  إيجابي  صادقة         NaN      1  \n",
            "\n",
            "[1600 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tokenizer and the Arabert model\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOR2u6wFD8fd",
        "outputId": "646a4882-38f9-4d3f-f701-8d46f800cbad"
      },
      "id": "ZOR2u6wFD8fd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_labeled = labeled_dataset[\"Review\"].values\n",
        "y_labeled = labeled_dataset[\"Label\"].values\n",
        "X_labeled_train, X_labeled_test, y_labeled_train, y_labeled_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "N4yQj7c3TXub"
      },
      "id": "N4yQj7c3TXub",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "def extract_features1(text):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    # Encode the input features with the model\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    # Extract the embeddings for the [CLS] token\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    \n",
        "    return embeddings.detach().numpy()\n",
        "\n",
        "\n",
        "def extract_features2(text):\n",
        "    # Tokenize the text with a different tokenizer\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    # Encode the input features with a different model\n",
        "    outputs = model(**inputs)\n",
        "    \n",
        "    # Extract the embeddings for the [CLS] token\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "    \n",
        "    return embeddings.detach().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "YvR0g4CiPCRP"
      },
      "id": "YvR0g4CiPCRP",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unlabeled_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIOtPD30Mcuq",
        "outputId": "e5d7affe-e835-4b2d-85fa-4ebed21d81a3"
      },
      "id": "CIOtPD30Mcuq",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Review  Class    Hotel Name  \\\n",
            "0    أنصح بالنوم وليس تناول الطعام  موقع مثالي للإق...   -1.0           NaN   \n",
            "1    كانت الغرفة ممتازة وكذلك الموظفون وبوفيه الإفط...   -1.0           NaN   \n",
            "2    فندق يتميز بمرافق نوعيّة وخلّاقة وساخنة. قم بش...   -1.0           NaN   \n",
            "3    لمسة بحرية  جميل وظريفة، فندق تقليدي في المدين...   -1.0           NaN   \n",
            "4    سأوصي بالتأكيد بموقع المدينة القديمة إلا إنه ع...   -1.0           NaN   \n",
            "..                                                 ...    ...           ...   \n",
            "794  Lemya تجعل إقامتي دائمًا مميزة. مفيد للغاية وم...    NaN  Radisson Blu   \n",
            "795  تجربة لا تنسى حقا. فندق من الدرجة الأولى يقع ف...    NaN  Radisson Blu   \n",
            "796  أنا أحب هذا الفندق الكثير من الرسوم المتحركة ا...    NaN  Radisson Blu   \n",
            "797  لإعطاء بعض السياق ، لقد حجزت 3 غرف:  الغرفة 1 ...    NaN  Radisson Blu   \n",
            "798  أردنا البقاء في الفندق (لأننا مسلمون) سألنا عن...    NaN  Radisson Blu   \n",
            "\n",
            "                           Review Title  \n",
            "0                                   NaN  \n",
            "1                                   NaN  \n",
            "2                                   NaN  \n",
            "3                                   NaN  \n",
            "4                                   NaN  \n",
            "..                                  ...  \n",
            "794  تجربة مثالية كما هو الحال دائمًا x  \n",
            "795                           مكان رائع  \n",
            "796                           فندق رائع  \n",
            "797                       تعريف الكارثة  \n",
            "798                                رهيب  \n",
            "\n",
            "[5601 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two different feature sets\n",
        "X_feat1_unlabeled = extract_features1(unlabeled_dataset)\n",
        "X_feat2_unlabeled = extract_features2(unlabeled_dataset)"
      ],
      "metadata": {
        "id": "dRJ7rDa1PCER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c2e4f37d-d776-468d-e400-9d99306eb8a2"
      },
      "id": "dRJ7rDa1PCER",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bffd8b0ab53b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define two different feature sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_feat1_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_feat2_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-bffd8b0ab53b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define two different feature sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_feat1_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_feat2_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train two classifiers on different feature sets\n",
        "clf1 = LogisticRegression()\n",
        "clf2 = LogisticRegression()\n",
        "\n",
        "X_feat1_train = extract_features1(X_labeled_train)\n",
        "X_feat2_train = extract_features2(X_labeled_train)\n",
        "\n",
        "clf1.fit(X_feat1_train, y_labeled_train)\n",
        "clf2.fit(X_feat2_train, y_labeled_train)"
      ],
      "metadata": {
        "id": "1qqCjBAAPFDI"
      },
      "id": "1qqCjBAAPFDI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 3\n",
        "# Iterate over the co-training process\n",
        "for i in range(num_iterations):\n",
        "    # Generate pseudo-labeled data using the two classifiers\n",
        "    X_feat1_unlabeled = extract_features1(unlabeled_dataset)\n",
        "    X_feat2_unlabeled = extract_features2(unlabeled_dataset)\n",
        "\n",
        "    y_pseudo1 = clf1.predict(X_feat1_unlabeled)\n",
        "    y_pseudo2 = clf2.predict(X_feat2_unlabeled)\n",
        "\n",
        "    # Select the most confident examples from each classifier\n",
        "    conf1 = np.max(clf1.predict_proba(X_feat1_unlabeled), axis=1)\n",
        "    conf2 = np.max(clf2.predict_proba(X_feat2_unlabeled), axis=1)\n",
        "\n",
        "    ind1 = np.argsort(-conf1)[:num_select]\n",
        "    ind2 = np.argsort(-conf2)[:num_select]\n",
        "\n",
        "    X_selected1 = X_feat1_unlabeled[ind1]\n",
        "    y_selected1 = y_pseudo1[ind1]\n",
        "\n",
        "    X_selected2 = X_feat2_unlabeled[ind2]\n",
        "    y_selected2 = y_pseudo2[ind2]\n",
        "\n",
        "    # Retrain the classifiers on the labeled data and selected data\n",
        "    X_feat1_labeled = np.concatenate([X_feat1_train, X_selected1])\n",
        "    y_labeled1 = np.concatenate([y_labeled_train, y_selected1])\n",
        "    clf1.fit(X_feat1_labeled, y_labeled1)\n",
        "\n",
        "    X_feat2_labeled = np.concatenate([X_feat2_train, X_selected2])\n",
        "    y_labeled2 = np.concatenate([y_labeled_train, y_selected2])\n",
        "    clf2.fit(X_feat2_labeled, y_labeled2)\n",
        "\n",
        "    # Evaluate the performance of the co-trained classifiers\n",
        "    X_feat1_test = extract_features1(X_labeled_test)\n",
        "    X_feat2_test = extract_features2(X_labeled_test)\n",
        "\n",
        "    y_pred1 = clf1.predict(X_feat1_test)\n",
        "    y_pred2 = clf2.predict(X_feat2_test)\n",
        "\n",
        "    y_ensemble = np.where((y_pred1 == y_pred2), y_pred1, np.nan)\n",
        "\n",
        "    acc_ensemble = accuracy_score(y_labeled_test, y_ensemble)\n",
        "    print(f\"Iteration {i+1}, ensemble accuracy: {acc_ensemble:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "JpbeAjAePExj",
        "outputId": "f0c8869b-aebf-47cb-f484-362d9dbd4607"
      },
      "id": "JpbeAjAePExj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-03558edbafaa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate pseudo-labeled data using the two classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX_feat1_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX_feat2_unlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c3fefd5773ce>\u001b[0m in \u001b[0;36mextract_features1\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Encode the input features with the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2589\u001b[0m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "73cec5994d92712fcda2ca8ea57ce6c0e579c55ad7f5ab9d7dbfd99192d6d4d7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}